diff --git a/.gitignore b/.gitignore
index 238fb3a200..521baaa795 100644
--- a/.gitignore
+++ b/.gitignore
@@ -16,3 +16,4 @@ tests/coverage_html/
 tests/.coverage
 build/
 tests/report/
+.aider/
diff --git a/django/contrib/staticfiles/storage.py b/django/contrib/staticfiles/storage.py
index 494890cc23..7bb9370963 100644
--- a/django/contrib/staticfiles/storage.py
+++ b/django/contrib/staticfiles/storage.py
@@ -52,10 +52,18 @@ class HashedFilesMixin:
     )
     keep_intermediate_files = True
 
+    @property
+    def hashed_files(self):
+        return getattr(self, '_hashed_files', {})
+
+    @hashed_files.setter
+    def hashed_files(self, value):
+        self._hashed_files = dict(value) if value else {}
+
     def __init__(self, *args, **kwargs):
+        self._hashed_files = {}  # Use private attribute
         super().__init__(*args, **kwargs)
         self._patterns = {}
-        self.hashed_files = {}
         for extension, patterns in self.patterns:
             for pattern in patterns:
                 if isinstance(pattern, (tuple, list)):
@@ -218,35 +226,43 @@ class HashedFilesMixin:
         if dry_run:
             return
 
-        # where to store the new paths
-        hashed_files = {}
+        # Reset hashed files
+        self._hashed_files = {}
+
+        # Track which files have been yielded
+        yielded_files = set()
 
         # build a list of adjustable files
         adjustable_paths = [
             path for path in paths
             if matches_patterns(path, self._patterns)
         ]
-        # Do a single pass first. Post-process all files once, then repeat for
-        # adjustable files.
-        for name, hashed_name, processed, _ in self._post_process(paths, adjustable_paths, hashed_files):
-            yield name, hashed_name, processed
-
-        paths = {path: paths[path] for path in adjustable_paths}
 
-        for i in range(self.max_post_process_passes):
-            substitutions = False
-            for name, hashed_name, processed, subst in self._post_process(paths, adjustable_paths, hashed_files):
-                yield name, hashed_name, processed
-                substitutions = substitutions or subst
-
-            if not substitutions:
-                break
-
-        if substitutions:
-            yield 'All', None, RuntimeError('Max post-process passes exceeded.')
-
-        # Store the processed paths
-        self.hashed_files.update(hashed_files)
+        # First pass: Process all non-adjustable files
+        non_adjustable = {path: paths[path] for path in paths if path not in adjustable_paths}
+        if non_adjustable:
+            for name, hashed_name, processed, _ in self._post_process(non_adjustable, set(), self._hashed_files):
+                if processed and name not in yielded_files:
+                    yielded_files.add(name)
+                    yield name, hashed_name, processed
+
+        # Process adjustable files
+        if adjustable_paths:
+            adjustable_files = {path: paths[path] for path in adjustable_paths}
+            for i in range(self.max_post_process_passes):
+                substitutions = False
+                
+                for name, hashed_name, processed, subst in self._post_process(adjustable_files, adjustable_paths, self._hashed_files):
+                    substitutions = substitutions or subst
+                    if processed and name not in yielded_files:
+                        yielded_files.add(name)
+                        yield name, hashed_name, processed
+
+                if not substitutions:
+                    break
+
+            if substitutions:
+                yield 'All', None, RuntimeError('Max post-process passes exceeded.')
 
     def _post_process(self, paths, adjustable_paths, hashed_files):
         # Sort the files by directory level
@@ -254,7 +270,7 @@ class HashedFilesMixin:
             return len(name.split(os.sep))
 
         for name in sorted(paths, key=path_level, reverse=True):
-            substitutions = True
+            substitutions = False  # Initialize substitutions flag
             # use the original, local file, not the copied-but-unprocessed
             # file, which might be somewhere far away, like S3
             storage, path = paths[name]
@@ -262,50 +278,75 @@ class HashedFilesMixin:
                 cleaned_name = self.clean_name(name)
                 hash_key = self.hash_key(cleaned_name)
 
-                # generate the hash with the original content, even for
-                # adjustable files.
-                if hash_key not in hashed_files:
-                    hashed_name = self.hashed_name(name, original_file)
-                else:
+                processed = False
+                # First, check if we've already processed this file
+                if hash_key in hashed_files:
                     hashed_name = hashed_files[hash_key]
+                    # Only process adjustable files in subsequent passes
+                    needs_processing = name in adjustable_paths
+                else:
+                      # New file, needs initial processing
+                      if hash_key not in self._hashed_files:
+                          hashed_name = self.hashed_name(name, original_file)
+                          hashed_file_exists = self.exists(hashed_name)
+                          if not hashed_file_exists:
+                              saved_name = self._save(hashed_name, original_file)
+                              hashed_name = self.clean_name(saved_name)
+                          # Update hashed_files for new files
+                          hashed_files[hash_key] = hashed_name
+                          self._hashed_files[hash_key] = hashed_name
+                          processed = True
+                      needs_processing = name in adjustable_paths  # Only process adjustable files
 
                 # then get the original's file content..
                 if hasattr(original_file, 'seek'):
                     original_file.seek(0)
 
+                # Check if the hashed file exists before content processing
                 hashed_file_exists = self.exists(hashed_name)
-                processed = False
 
-                # ..to apply each replacement pattern to the content
-                if name in adjustable_paths:
+                # Process content if needed (for adjustable files)
+                if name in adjustable_paths and needs_processing:
                     old_hashed_name = hashed_name
                     content = original_file.read().decode('utf-8')
+                    content_changed = False
                     for extension, patterns in self._patterns.items():
                         if matches_patterns(path, (extension,)):
                             for pattern, template in patterns:
                                 converter = self.url_converter(name, hashed_files, template)
                                 try:
-                                    content = pattern.sub(converter, content)
+                                    new_content = pattern.sub(converter, content)
+                                    if new_content != content:
+                                        content = new_content
+                                        content_changed = True
                                 except ValueError as exc:
                                     yield name, None, exc, False
-                    if hashed_file_exists:
-                        self.delete(hashed_name)
-                    # then save the processed result
-                    content_file = ContentFile(content.encode())
-                    if self.keep_intermediate_files:
-                        # Save intermediate file for reference
-                        self._save(hashed_name, content_file)
-                    hashed_name = self.hashed_name(name, content_file)
-
-                    if self.exists(hashed_name):
-                        self.delete(hashed_name)
-
-                    saved_name = self._save(hashed_name, content_file)
-                    hashed_name = self.clean_name(saved_name)
-                    # If the file hash stayed the same, this file didn't change
-                    if old_hashed_name == hashed_name:
-                        substitutions = False
-                    processed = True
+
+                    if content_changed:
+                        if hashed_file_exists:
+                            self.delete(hashed_name)
+                        # then save the processed result
+                        content_file = ContentFile(content.encode())
+                        if self.keep_intermediate_files:
+                            # Save intermediate file for reference
+                            self._save(hashed_name, content_file)
+                        hashed_name = self.hashed_name(name, content_file)
+
+                        if self.exists(hashed_name):
+                            self.delete(hashed_name)
+
+                        saved_name = self._save(hashed_name, content_file)
+                        hashed_name = self.clean_name(saved_name)
+                        # If the file hash stayed the same, this file didn't change
+                        if old_hashed_name == hashed_name:
+                            substitutions = False
+                            processed = False  # Don't mark as processed if hash didn't change
+                        else:
+                            substitutions = True
+                            processed = True  # Mark as processed only if content changed
+                            # Update both local and instance hashed_files
+                            hashed_files[hash_key] = hashed_name
+                            self._hashed_files[hash_key] = hashed_name
 
                 if not processed:
                     # or handle the case in which neither processing nor
@@ -314,9 +355,9 @@ class HashedFilesMixin:
                         processed = True
                         saved_name = self._save(hashed_name, original_file)
                         hashed_name = self.clean_name(saved_name)
-
-                # and then set the cache accordingly
-                hashed_files[hash_key] = hashed_name
+                        # Update both local and instance hashed_files
+                        hashed_files[hash_key] = hashed_name
+                        self.hashed_files[hash_key] = hashed_name
 
                 yield name, hashed_name, processed, substitutions
