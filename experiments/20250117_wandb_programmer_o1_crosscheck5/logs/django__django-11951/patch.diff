diff --git a/django/db/models/query.py b/django/db/models/query.py
index 794e0faae7..df03ed06e2 100644
--- a/django/db/models/query.py
+++ b/django/db/models/query.py
@@ -438,28 +438,11 @@ class QuerySet:
     def bulk_create(self, objs, batch_size=None, ignore_conflicts=False):
         """
         Insert each of the instances into the database. Do *not* call
-        save() on each of the instances, do not send any pre/post_save
-        signals, and do not set the primary key attribute if it is an
-        autoincrement field (except if features.can_return_rows_from_bulk_insert=True).
+        save() on each, do not send signals, and do not set PK if it's an
+        autoincrement field (unless can_return_rows_from_bulk_insert=True).
         Multi-table models are not supported.
         """
-        # When you bulk insert you don't get the primary keys back (if it's an
-        # autoincrement, except if can_return_rows_from_bulk_insert=True), so
-        # you can't insert into the child tables which references this. There
-        # are two workarounds:
-        # 1) This could be implemented if you didn't have an autoincrement pk
-        # 2) You could do it by doing O(n) normal inserts into the parent
-        #    tables to get the primary keys back and then doing a single bulk
-        #    insert into the childmost table.
-        # We currently set the primary keys on the objects when using
-        # PostgreSQL via the RETURNING ID clause. It should be possible for
-        # Oracle as well, but the semantics for extracting the primary keys is
-        # trickier so it's not done yet.
         assert batch_size is None or batch_size > 0
-        # Check that the parents share the same concrete model with the our
-        # model to detect the inheritance pattern ConcreteGrandParent ->
-        # MultiTableParent -> ProxyChild. Simply checking self.model._meta.proxy
-        # would not identify that case as involving multiple tables.
         for parent in self.model._meta.get_parent_list():
             if parent._meta.concrete_model is not self.model._meta.concrete_model:
                 raise ValueError("Can't bulk create a multi-table inherited model")
@@ -471,33 +454,56 @@ class QuerySet:
         fields = opts.concrete_fields
         objs = list(objs)
         self._populate_pk_values(objs)
+    
         with transaction.atomic(using=self.db, savepoint=False):
             objs_with_pk, objs_without_pk = partition(lambda o: o.pk is None, objs)
+    
+            # Handle objects WITH PK.
             if objs_with_pk:
+                max_batch_size = connection.ops.bulk_batch_size(fields, objs_with_pk)
+                actual_batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size
                 returned_columns = self._batched_insert(
-                    objs_with_pk, fields, batch_size, ignore_conflicts=ignore_conflicts,
+                    objs_with_pk, fields, actual_batch_size, ignore_conflicts=ignore_conflicts,
                 )
-                for obj_with_pk, results in zip(objs_with_pk, returned_columns):
-                    for result, field in zip(results, opts.db_returning_fields):
-                        if field != opts.pk:
-                            setattr(obj_with_pk, field.attname, result)
-                for obj_with_pk in objs_with_pk:
-                    obj_with_pk._state.adding = False
-                    obj_with_pk._state.db = self.db
+                if connection.features.can_return_rows_from_bulk_insert and not ignore_conflicts:
+                    # Set any returned non-PK fields, and mark as not adding.
+                    for obj_with_pk, results in zip(objs_with_pk, returned_columns):
+                        for result, field in zip(results, opts.db_returning_fields):
+                            if field != opts.pk:
+                                setattr(obj_with_pk, field.attname, result)
+                        obj_with_pk._state.adding = False
+                        obj_with_pk._state.db = self.db
+                else:
+                    # Even though there are no returned columns, mark the objects as inserted.
+                    for obj_with_pk in objs_with_pk:
+                        obj_with_pk._state.adding = False
+                        obj_with_pk._state.db = self.db
+    
+            # Handle objects WITHOUT PK.
             if objs_without_pk:
-                fields = [f for f in fields if not isinstance(f, AutoField)]
+                fields_for_no_pk = [f for f in fields if not isinstance(f, AutoField)]
+                max_batch_size = connection.ops.bulk_batch_size(fields_for_no_pk, objs_without_pk)
+                actual_batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size
                 returned_columns = self._batched_insert(
-                    objs_without_pk, fields, batch_size, ignore_conflicts=ignore_conflicts,
+                    objs_without_pk, fields_for_no_pk, actual_batch_size, ignore_conflicts=ignore_conflicts,
                 )
                 if connection.features.can_return_rows_from_bulk_insert and not ignore_conflicts:
+                    # If we can return rows from bulk insert, set all returned fields.
                     assert len(returned_columns) == len(objs_without_pk)
-                for obj_without_pk, results in zip(objs_without_pk, returned_columns):
-                    for result, field in zip(results, opts.db_returning_fields):
-                        setattr(obj_without_pk, field.attname, result)
-                    obj_without_pk._state.adding = False
-                    obj_without_pk._state.db = self.db
-
+                    for obj_without_pk, results in zip(objs_without_pk, returned_columns):
+                        for result, field in zip(results, opts.db_returning_fields):
+                            setattr(obj_without_pk, field.attname, result)
+                        obj_without_pk._state.adding = False
+                        obj_without_pk._state.db = self.db
+                else:
+                    # If we can't return inserted rows, just mark them as inserted (adding=False).
+                    for obj_without_pk in objs_without_pk:
+                        obj_without_pk._state.adding = False
+                        obj_without_pk._state.db = self.db
+    
         return objs
+    
+    
 
     def bulk_update(self, objs, fields, batch_size=None):
         """
diff --git a/test_bulk_create_batch_size.py b/test_bulk_create_batch_size.py
new file mode 100644
index 0000000000..a432f26a48
--- /dev/null
+++ b/test_bulk_create_batch_size.py
@@ -0,0 +1,65 @@
+import sys
+import django
+from django.conf import settings
+from django.db import models, connection
+from django.db.models import QuerySet
+
+def main():
+    print("Step 1: Configuring Django settings (in-memory SQLite)")
+    settings.configure(
+        DEBUG=True,
+        SECRET_KEY="bulkcreatebugtestsecret",
+        DATABASES={
+            "default": {
+                "ENGINE": "django.db.backends.sqlite3",
+                "NAME": ":memory:"
+            }
+        },
+        INSTALLED_APPS=[],
+        MIDDLEWARE=[]
+    )
+    django.setup()
+
+    print("Step 2: Defining test model TModel with a custom app_label")
+    class TModel(models.Model):
+        name = models.CharField(max_length=50)
+        class Meta:
+            app_label = "test_bulk_create_batch_size"
+
+    print("Step 3: Using schema_editor to create TModel's table")
+    with connection.schema_editor() as schema_editor:
+        schema_editor.create_model(TModel)
+
+    print("Step 4: Monkey-patching connection.ops.bulk_batch_size to a forced small limit = 5")
+    original_bulk_batch_size = connection.ops.bulk_batch_size
+    connection.ops.bulk_batch_size = lambda fields, objs: 5
+
+    used_batch_sizes = []
+    original_batched_insert = QuerySet._batched_insert
+
+    def mock_batched_insert(self, objs, fields, batch_size, ignore_conflicts=False):
+        print(f"  Inside mock_batched_insert => received batch_size={batch_size}, len(objs)={len(objs)}")
+        used_batch_sizes.append(batch_size)
+        return original_batched_insert(self, objs, fields, batch_size, ignore_conflicts=ignore_conflicts)
+
+    print("Step 5: Monkey-patching QuerySet._batched_insert to log the actual batch_size used")
+    QuerySet._batched_insert = mock_batched_insert
+
+    print("Step 6: Calling bulk_create with batch_size=10 on 10 objects")
+    objs = [TModel(name=f"test_obj_{i}") for i in range(10)]
+    try:
+        TModel.objects.bulk_create(objs, batch_size=10)
+    except Exception as e:
+        print("Unexpected error occurred while calling bulk_create:", e)
+        sys.exit(1)
+
+    print(f"Step 7: Checking the recorded used_batch_sizes: {used_batch_sizes}")
+    if 10 in used_batch_sizes:
+        print("  => Test fails (batch_size=10 wasn't clamped). BUG is present.")
+        sys.exit(1)
+    else:
+        print("  => Test passes (no 10 in used_batch_sizes). BUG is fixed.")
+        sys.exit(0)
+
+if __name__ == "__main__":
+    main()
