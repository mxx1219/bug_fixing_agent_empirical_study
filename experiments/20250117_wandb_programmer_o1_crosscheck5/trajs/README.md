# How to read these trajectories.

First, let me describe the pipeline for this submission:

Step 1: testCommand [testCommandTrajectory]

- This step uses a gpt4o based agent to determine a canonical command for running tests within a given repo. You can look for the task_done tool call at the end of the trajectory to see what the output looks like.

Step 2: 5 solution rollouts [solutionTrajectories]

- In this step we use the o1 agent to generate 5 solutions to the task. The environment automatically commits any oustanding git changes prior to the agent run. The final patch is whatever "git add . && git diff" returns.

Step 3: crosscheck [crosscheckResult]

- No agent is used in the crosscheck step, but there may be some LLM calls made. Here's a summary of how it works:
  - first, cross-check only considers trajectories where the agent did not return an error or hit the token limit.
  - for each trajectory, the system splits the final patch into two parts, new and existing, where new is the subset of the patch that contains new files, and existing is the subset of the patch that contains anything else.
  - the system looks at any auto-commands registered within each trajectory, and figures out which autocommands are for new tests v. existing (by looking at the intersection of the command and the names of files in the new patch).
  - the system then runs cross-checks all the tests from each trajectory on each other trajectory's patch, using the assets generated above. it runs the tests before the fix patch is applied and after for each.
  - new tests are only correct if they fail before the fix patch is applied and pass after the fix patch is applied.
  - for existing tests, the system uses an LLM call on the test logs to determine if there were regressions (by passing both before and after results)
  - all of this is tallied up into a simple score for each trajectory.
  - if there is a tie, the system uses a single o1 call, passing in the patches and the problem statement as context. The prompt for that final call can be seen in the trajectories. It just presents the problem and patches and asks for a chosenSolutionId
- The crossCheckResult in the trajectories shows the results of each of the steps described above.

Note: I did an expediency/cost hack for the sake of this benchmark. I only ran cross-check on any instance that did not have either all pass or all fail (based on my implementation of the swebench scorer). This is a valid shortcut because picking any one of an all pass instance results in pass, and likewise for all fail. So instances that have no crossCheckResult are either all pass or all fail. At the end, for solutions that were all pass or all fail, I just pick the first solution. Same for the three instances that had cross-check errors. The rest are whatever solutions were chosen by cross-check.

I think cross-check is interesting and simple. There are also lots of ways to improve it. If folks find it useful, and its not well known in existing literature, I'd love to be cited for it. Of course, maybe everyone does this and I just don't know. I'm not familiar with what's out there!

Here's how to get in touch: shawn@wandb.com
