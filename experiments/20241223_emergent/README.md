# Emergent E1 Code Agent

At Emergent, we are building general coding agents capable of adapting to a wide range of coding tasks. Our microagent architecture allows the delegation of responsibility to task-specific agents and results in superior end-to-end task completion rates.

## SWE-bench Results

We demonstrate the success of our agent by using our system to attempt the SWE-Bench Verified problem set. The following table shows the problems resolved by our system.

| Total Instances | Patch Generated |   Resolved | Resolved Rate |
| --------------- | --------------- |   ---------| ------------- |
| 500 | 500 |  286 | 57.20% |

## Approach

Our approach is based on a multiagent architecture where each agent is task specific. In the context of the SWE-bench problem, before initiating problem-solving, we have a search agent that enriches the issue with additional context from the repository. Likewise, edits requested by the agent are handled by an Edit agent. We also enhance the agent with the ability to run tests. Finally, we have built an LLM-based agentic patch adjudicator which takes in trajectories and patches, and judges the best patch for a problem statement.


## Notes
* Our current version of the E1 agent relies on a combination of Claude 3.5 Sonnet (version dated 2024-06-20, an older model) and GPT-4o to achieve state-of-the-art results on the SWE-Bench Verified problem set. We are excited to try out the newer Claude models and O-1, but the current results do not utilize the new Sonnet or O-1 models.
* Our submission is pass@1, that is, we only ever attempt a problem once. Our attempt itself does consist of multiple runs of (possibly different agents) but the selection of the final submission is part of this single run and is done by our patch selection module.
* We do not use any SWE-bench test knowledge nor do we use any hints as part of our solution. Our patch selection module makes use of results of test runs to adjudicate patches , but these tests are independently discovered using the repository structure, issue details, and the agent’s trajectories.
* We built on top of SWE-Agent with custom agents for search, edit, diffs, and a few additional services.

## Checklist

[✅] Is a pass@1 submission (does not attempt the same task instance more than once)


[✅] Does not use SWE-bench test knowledge (PASS_TO_PASS, FAIL_TO_PASS)


[✅] Does not use the hints field in SWE-bench


[✅] Does not have web-browsing OR has taken steps to prevent lookup of SWE-bench solutions via web-browsing
